{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step guide to run Whisper inference with VLLM\n",
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import WhisperTokenizerFast\n",
    "from pathlib import Path\n",
    "from librosa import resample, load\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since whisper can process only up to 30 seconds of audio, we might need to chunk audio. \n",
    "\n",
    "Example of chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(audio:np.ndarray, sample_rate:int):\n",
    "    \"\"\"Split audio to 30 second duration chunks\n",
    "\n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data\n",
    "        sample_rate (int): Audio sample rate\n",
    "    \"\"\"\n",
    "    max_duration_samples = sample_rate * 30.0\n",
    "    padding = max_duration_samples - np.remainder(len(audio), max_duration_samples)\n",
    "    audio = np.pad(audio, (0, padding.astype(int)), 'constant', constant_values=0.0)\n",
    "    return np.split(audio, len(audio) // max_duration_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define VLLM engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper = LLM(\n",
    "        model=\"openai/whisper-large-v3-turbo\",\n",
    "        limit_mm_per_prompt={\"audio\": 1},\n",
    "        gpu_memory_utilization = 0.5,\n",
    "        dtype = \"float16\",\n",
    "        max_num_seqs = 4,\n",
    "        max_num_batched_tokens=448\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what what arguments we used\n",
    "\n",
    "1. `model` - is obviously the model we are using, it can be any Whisper realization from HuggingFace hub\n",
    "2. `limit_mm_per_prompt` - always like that because Whisper can only process 1 audio per prompt\n",
    "3. `gpu_memory_utilization` - is the fraction of memory that VLLM can use for model inference and KV caching.\n",
    "4. `dtype` - is the type of model weights, float16 or bfloat16 (for RTX 30xx series) is recommended for GPU\n",
    "5. `max_num_seqs` - basically is a batch size \n",
    "6. `max_num_batched_tokens` - is the maximum number of tokens that can be processed in a batch, this is a hard limit and should be set to 448 for Whisper (because of audio duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future flexibility we also want dynamically set language code. Without it, Whisper will try to translate the audio to English. Check [Whisper documentation](https://platform.openai.com/docs/guides/speech-to-text/supported-languages/#supported-languages) to find supported languages list.\n",
    "\n",
    "<details>\n",
    "<summary>The full list with codes</summary>\n",
    "\n",
    "    'afrikaans': 'af',\n",
    "    'arabic': 'ar',\n",
    "    'armenian': 'hy',\n",
    "    'azerbaijani': 'az',\n",
    "    'belarusian': 'be',\n",
    "    'bosnian': 'bs',\n",
    "    'bulgarian': 'bg',\n",
    "    'catalan': 'ca',\n",
    "    'chinese': 'zh',\n",
    "    'croatian': 'hr',\n",
    "    'czech': 'cs',\n",
    "    'danish': 'da',\n",
    "    'dutch': 'nl',\n",
    "    'english': 'en',\n",
    "    'estonian': 'et',\n",
    "    'finnish': 'fi',\n",
    "    'french': 'fr',\n",
    "    'galician': 'gl',\n",
    "    'german': 'de',\n",
    "    'greek': 'el',\n",
    "    'hebrew': 'he',\n",
    "    'hindi': 'hi',\n",
    "    'hungarian': 'hu',\n",
    "    'icelandic': 'is',\n",
    "    'indonesian': 'id',\n",
    "    'italian': 'it',\n",
    "    'japanese': 'ja',\n",
    "    'kannada': 'kn',\n",
    "    'kazakh': 'kk',\n",
    "    'korean': 'ko',\n",
    "    'latvian': 'lv',\n",
    "    'lithuanian': 'lt',\n",
    "    'macedonian': 'mk',\n",
    "    'malay': 'ms',\n",
    "    'maori': 'mi',\n",
    "    'marathi': 'mr',\n",
    "    'nepali': 'ne',\n",
    "    'norwegian': 'no',\n",
    "    'persian': 'fa',\n",
    "    'polish': 'pl',\n",
    "    'portuguese': 'pt',\n",
    "    'romanian': 'ro',\n",
    "    'russian': 'ru',\n",
    "    'serbian': 'sr',\n",
    "    'slovak': 'sk',\n",
    "    'slovenian': 'sl',\n",
    "    'spanish': 'es',\n",
    "    'swahili': 'sw',\n",
    "    'swedish': 'sv',\n",
    "    'tagalog': 'tl',\n",
    "    'tamil': 'ta',\n",
    "    'thai': 'th',\n",
    "    'turkish': 'tr',\n",
    "    'ukrainian': 'uk',\n",
    "    'urdu': 'ur',\n",
    "    'vietnamese': 'vi',\n",
    "    'welsh': 'cy'\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-large-v3-turbo\", language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = \"ko\"\n",
    "lang_code = tokenizer.convert_tokens_to_ids(f\"<|{language}|>\")\n",
    "if lang_code == 50257:\n",
    "    raise ValueError(f\"Language code for {language} not found\")\n",
    "\n",
    "lang_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example suggest that audio samples is located in `samples` folder and have `wav` extention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = Path(\"samples\").glob(\"*.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper can only process audio with 16000Hz sample rate, so we need to convert it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: samples/sample.wav, Sample rate: 16000, Audio shape: (889352,), Duration: 55.58 seconds\n",
      "File: samples/sample2.wav, Sample rate: 16000, Audio shape: (1008610,), Duration: 63.04 seconds\n",
      "File: samples/sample3.wav, Sample rate: 16000, Audio shape: (596160,), Duration: 37.26 seconds\n",
      "File: samples/sample4.wav, Sample rate: 16000, Audio shape: (1848960,), Duration: 115.56 seconds\n"
     ]
    }
   ],
   "source": [
    "samples = {}\n",
    "\n",
    "for file in list(audio_files):\n",
    "    # Load the audio file\n",
    "    audio, sample_rate = load(file,sr=16000)\n",
    "    if sample_rate != 16000:\n",
    "        # Use librosa to resample the audio\n",
    "        audio = resample(audio.numpy().astype(np.float32), orig_sr=sample_rate, target_sr=16000)\n",
    "    print(f\"File: {file}, Sample rate: {sample_rate}, Audio shape: {audio.shape}, Duration: {audio.shape[0] / sample_rate:.2f} seconds\")\n",
    "    chunks = chunking(audio, 16000)\n",
    "    samples[file.stem] = [(chunk,16000) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important notice:\n",
    "\n",
    "VLLM expect to recieve a `tuple[np.ndarray, int]` where furst element is audio data and second is sample rate. We do exactly that in `samples[file.stem] = [(chunk,16000) for chunk in chunks]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ready to process the audio files.\n",
    "\n",
    "The inference loop can be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, chunks in samples.items():\n",
    "        \n",
    "    prompts = [{\n",
    "                \"encoder_prompt\": {\n",
    "                    \"prompt\": \"\",\n",
    "                    \"multi_modal_data\": {\n",
    "                        \"audio\": chunk,\n",
    "                    },\n",
    "                },\n",
    "                \"decoder_prompt\":\n",
    "                f\"<|startoftranscript|><|{lang_code}|><|transcribe|><|notimestamps|>\"\n",
    "            } for chunk in chunks]\n",
    "    print(f\"File: {file}, Chunks: {len(chunks)}\")\n",
    "    # Create a sampling params object.\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=8192,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Inferense based on max_num_seqs\n",
    "    outputs = []\n",
    "    for i in range(0, len(prompts)):\n",
    "        output = whisper.generate(prompts[i], sampling_params=sampling_params)\n",
    "        outputs.extend(output)\n",
    "    # Print the outputs.\n",
    "    generated = \"\"\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        encoder_prompt = output.encoder_prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        generated+= generated_text\n",
    "        print(f\"Encoder prompt: {encoder_prompt!r}, \"\n",
    "            f\"Decoder prompt: {prompt!r}, \"\n",
    "            f\"Generated text: {generated_text!r}\")\n",
    "\n",
    "    duration = time.time() - start\n",
    "\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"RPS:\", len(prompt) / duration)\n",
    "    print(\"Generated text:\", generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it step by step.\n",
    "\n",
    "### Prompting\n",
    "First, we need to create query according to the VLLM prompting format. There is actually 2 possible ways to do it for Whisper:\n",
    "1. We can use separate encoder and decoder prompts. In that case endoder receives the audio data and decoder receives the text prompt:\n",
    "```prompts = [\n",
    "    {\n",
    "        \"prompt\": f\"<|startoftranscript|><|{lang_code}|><|transcribe|><|notimestamps|>\",\n",
    "        \"multi_modal_data\": {\n",
    "            \"audio\": chunk,\n",
    "        }\n",
    "    } for chunk in chunks]\n",
    "]\n",
    "```\n",
    "2. We can use a single prompt where we provide the audio data and the text prompt as shown in code.\n",
    "\n",
    "In certain way, 1st method is more simple and clear, but 2nd still valid.\n",
    "\n",
    "The text prompt `<|startoftranscript|><|{lang_code}|><|transcribe|><|notimestamps|>` consist 4 tokens:\n",
    "\n",
    "1. `<|startoftranscript|>` is always present.\n",
    "2. `<|{lang_code}|>` - determine the language of transcribition. When actuall past to model, it'll look like `<|en|>` or `<|de|>`.\n",
    "3. `<|transcribe|>` - determine the task. Alternative is `<|translate|>`, which enables translation from source language to English. Such behaviour can also be reached by not specifying language token.\n",
    "4. `<|notimestamps|>` is optional token. If not provided, Whisper will also insert timestamp of the regoznized speech chunks. It might be useful for certain scenarios.\n",
    "\n",
    "Notice: VLLM expects that in ```\"multi_modal_data\": {\n",
    "            \"audio\": chunk,\n",
    "        }``` `chunk` is tuple (`np.ndarray`, `sample_rate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling parameters\n",
    "\n",
    "Usually, the sampling parameters are set to the following values:\n",
    "- `temperature`: 0\n",
    "- `top_p`: 1.0\n",
    "\n",
    "`max_tokens` is actually optional, but I recommend setting it to somrthing like 4096 or 8192. The reason is that VLLM has a very small default value (about 16 or so), and because of that Whisper can't properly proceed a 30 seconds chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing loop\n",
    "\n",
    "Is quite simple. In the example, it uses a pretty simple chink-by-chunk implementation, but it also possible to use a batched input like that:\n",
    "\n",
    "```for i in range(0, len(prompts), max_num_seqs):\n",
    "            output = whisper.generate(prompts[i:i+max_num_seqs], sampling_params=sampling_params)\n",
    "            outputs.extend(output)\n",
    "```\n",
    "\n",
    "In such way, you can increase your throughput even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
